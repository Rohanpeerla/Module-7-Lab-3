{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoCXtClNbPcyfKWA6HUuYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohanpeerla/Module-7-Lab-3/blob/master/Module_7_Lab_3_FMML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with Text 1 : Bag of Words Model"
      ],
      "metadata": {
        "id": "Tp3nmWFJeVNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-NQCCx8aLqJ",
        "outputId": "c0fb8c78-a07c-4a37-fb96-e81464f96466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Vocabulary:\n",
            "['grey', 'is', 'a', 'dull', 'colour', 'orange', 'fruit', 'this', 'new', 'sentence', 'another', 'example', 'of', 'text', 'data', 'machine', 'learning', 'interesting']\n",
            "\n",
            "BoW Representations for New Sentences:\n",
            "['this', 'is', 'a', 'new', 'sentence'] : [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['another', 'example', 'of', 'text', 'data'] : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "['machine', 'learning', 'is', 'interesting'] : [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    [\"grey\", \"is\", \"a\", \"dull\", \"colour\"],\n",
        "    [\"orange\", \"is\", \"a\", \"fruit\"],\n",
        "    [\"orange\", \"is\", \"a\", \"colour\"]\n",
        "]\n",
        "vocabulary = []\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "new_sentences = [\n",
        "    [\"this\", \"is\", \"a\", \"new\", \"sentence\"],\n",
        "    [\"another\", \"example\", \"of\", \"text\", \"data\"],\n",
        "    [\"machine\", \"learning\", \"is\", \"interesting\"]\n",
        "]\n",
        "sentences += new_sentences\n",
        "for sentence in new_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in vocabulary:\n",
        "            vocabulary.append(word)\n",
        "new_bow_sentences = []\n",
        "for sentence in new_sentences:\n",
        "    bow_representation = [0 for _ in range(len(vocabulary))]\n",
        "    for indx, word in enumerate(vocabulary):\n",
        "        if word in sentence:\n",
        "            bow_representation[indx] += 1\n",
        "    new_bow_sentences.append(bow_representation)\n",
        "print(\"Updated Vocabulary:\")\n",
        "print(vocabulary)\n",
        "print(\"\\nBoW Representations for New Sentences:\")\n",
        "for sentence, bow_rep in zip(new_sentences, new_bow_sentences):\n",
        "    print(sentence, \":\", bow_rep)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with Text 2 : LSI (Latent Semantic Indexing)"
      ],
      "metadata": {
        "id": "H821qcOEehcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "!curl -L -o 'movies.csv' 'https://drive.google.com/uc?export=downloads&id=12k4ltUwdhg525XW4dT6hgW18t_Fil3Hj'\n",
        "df = pd.read_csv('movies.csv', sep=',', usecols=['Release Year', 'Title', 'Plot'])\n",
        "df = df[df['Release Year'] >= 2000]  # Use a subset of the data\n",
        "text_corpus = df['Plot'].values\n",
        "processed_corpus = [preprocess_string(text) for text in text_corpus]\n",
        "dictionary = gensim.corpora.Dictionary(processed_corpus)\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
        "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "lsi = gensim.models.LsiModel(corpus_tfidf, num_topics=1000)\n",
        "index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])\n",
        "new_doc = \"In the arcade at night the video game characters leave their games. The protagonist is a girl from a candy racing game who glitches\"\n",
        "new_doc = preprocess_string(new_doc)\n",
        "new_vec = dictionary.doc2bow(new_doc)\n",
        "vec_bow_tfidf = tfidf[new_vec]\n",
        "vec_lsi = lsi[vec_bow_tfidf]\n",
        "sims = index[vec_lsi]\n",
        "print(\"Top 10 Recommended Movies:\")\n",
        "for s in sorted(enumerate(sims), key=lambda item: -item[1])[:10]:\n",
        "    print(f\"{df['Title'].iloc[s[0]]} : {str(s[1])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kmgGOjtch6o",
        "outputId": "9a971828-a653-4371-b22e-113e57021cda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 77.4M  100 77.4M    0     0  18.5M      0  0:00:04  0:00:04 --:--:-- 19.6M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Recommended Movies:\n",
            "Inferno : 0.62439\n",
            "Kami-sama no Iu Toori : 0.6176101\n",
            "Overlord: The Undead King : 0.5767039\n",
            "Overlord: The Dark Warrior : 0.5767039\n",
            "Wreck-It Ralph : 0.54785764\n",
            "5150 Elm's Way : 0.53625906\n",
            "How to Make a Monster : 0.5281404\n",
            "Stay Alive : 0.50853574\n",
            "Candlestick : 0.5053647\n",
            "Jumanji: Welcome to the Jungle : 0.46112248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.How exactly does matrix factorization help us in the recommendation procedure? Why can we not simply model the user-movie matrix?\n"
      ],
      "metadata": {
        "id": "vWp35KoRdGaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix factorization plays a crucial role in recommendation systems, especially collaborative filtering-based methods. Here's how matrix factorization helps in the recommendation procedure and why modeling the user-movie matrix directly may not be the most effective approach:\n",
        "\n",
        "1. **Dimensionality Reduction**: Matrix factorization techniques like Singular Value Decomposition (SVD) or Alternating Least Squares (ALS) decompose the original user-item interaction matrix into lower-dimensional matrices. By reducing the dimensionality, we can capture the underlying latent factors that contribute to user preferences and item characteristics more effectively.\n",
        "\n",
        "2. **Handling Sparsity**: User-movie matrices in recommendation systems are often sparse, meaning that most entries are missing because users have not rated most items. Matrix factorization allows us to approximate the original sparse matrix with a denser representation, making it easier to make predictions for missing entries.\n",
        "\n",
        "3. **Discovering Latent Features**: Matrix factorization uncovers latent features that represent underlying characteristics of users and items. These latent features may not be directly observable but capture meaningful patterns in the data, such as genres, themes, or user preferences. By learning these latent features, we can make better recommendations by identifying similarities between users and items based on shared latent factors.\n",
        "\n",
        "4. **Scalability**: Modeling the entire user-movie matrix directly can be computationally expensive and memory-intensive, especially for large-scale datasets with millions of users and items. Matrix factorization techniques provide a more scalable approach by decomposing the matrix into smaller matrices, which can be computed and stored more efficiently.\n",
        "\n",
        "5. **Personalization**: Matrix factorization allows for personalized recommendations by learning unique user and item embeddings. Each user and item is represented by a vector of latent factors, capturing individual preferences and characteristics. This enables the recommendation system to tailor recommendations to each user's specific tastes and preferences.\n",
        "\n",
        "6. **Flexibility**: Matrix factorization techniques offer flexibility in incorporating additional information, such as user demographics, item attributes, or contextual factors, into the recommendation model. By integrating such information into the factorization process, we can enhance the quality and relevance of recommendations.\n",
        "\n",
        "Overall, matrix factorization techniques provide a powerful framework for recommendation systems by reducing dimensionality, handling sparsity, discovering latent features, improving scalability, enabling personalization, and offering flexibility to incorporate various types of information. These advantages make matrix factorization a key component in modern recommendation algorithms."
      ],
      "metadata": {
        "id": "vNYCWHZVdQ-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What do the rows of the matrix  T  represent? (Definition of  T  is above in the introduction to LSI)."
      ],
      "metadata": {
        "id": "1YZQBpDOdTr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Latent Semantic Indexing (LSI), the matrix \\( T \\) represents the term-concept vector matrix. Each row of the matrix \\( T \\) corresponds to a term (word) in the vocabulary of the corpus, and each column represents a latent concept or dimension discovered by the LSI algorithm.\n",
        "\n",
        "Specifically:\n",
        "\n",
        "- Each row \\( i \\) of matrix \\( T \\) corresponds to the \\( i \\)-th term (word) in the vocabulary.\n",
        "- The \\( j \\)-th element of row \\( i \\) in matrix \\( T \\) (\\( T_{ij} \\)) represents the weight or strength of the association between the \\( i \\)-th term and the \\( j \\)-th latent concept.\n",
        "\n",
        "In other words, each row of matrix \\( T \\) represents the vectorial representation of a term in the lower-dimensional latent concept space discovered by LSI. These vectors capture the semantic meaning or contextual information of each term based on its relationships with the latent concepts derived from the corpus.\n",
        "\n",
        "1.Semantic Representation: Each row of the matrix \\( T \\) provides a semantic representation of a term (word) in the vocabulary of the corpus. These representations capture the underlying meaning or semantic relationships of terms based on their co-occurrence patterns within the corpus.\n",
        "\n",
        "2. Dimensionality Reduction: The rows of matrix \\( T \\) are typically of lower dimensionality compared to the original term space. This reduction in dimensionality is achieved through the singular value decomposition (SVD) process, where the original high-dimensional term space is projected onto a lower-dimensional latent concept space.\n",
        "\n",
        "3. Latent Concepts: The columns of matrix \\( T \\) represent latent concepts or dimensions discovered by the LSI algorithm. Each row of \\( T \\) is a vector that represents how the corresponding term is related to these latent concepts. These latent concepts are not directly observable but capture underlying patterns or themes in the corpus.\n",
        "\n",
        "4. Semantic Similarity: Similar terms in the corpus tend to have similar representations in matrix \\( T \\), meaning they are close to each other in the latent concept space. This allows LSI to capture semantic similarity between terms and enables the model to generalize well to unseen terms.\n",
        "\n",
        "5. Term Importance: The values in each row of matrix \\( T \\) indicate the importance or relevance of each term to the latent concepts. Higher values indicate stronger associations between the term and the corresponding latent concept, while lower values suggest weaker associations.\n",
        "\n",
        "6. Interpretability: By examining the values in the rows of matrix \\( T \\), it is possible to interpret the semantic meaning of each term in the context of the discovered latent concepts. This allows for better understanding of the relationships between terms and facilitates tasks such as information retrieval and document clustering.\n",
        "\n",
        "In summary, the rows of the matrix \\( T \\) in LSI provide a compact and semantically meaningful representation of terms in the corpus, capturing their relationships with latent concepts discovered through dimensionality reduction techniques like SVD. These representations enable LSI to effectively model and analyze the semantic structure of textual data."
      ],
      "metadata": {
        "id": "CRj6ZrWndh-1"
      }
    }
  ]
}